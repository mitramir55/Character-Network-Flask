{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from crypt import methods\n",
    "from distutils.command.config import config\n",
    "from flask import Flask, flash, redirect, url_for, render_template, request, Response\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests, json, ast\n",
    "import os, io, csv, sys, pickle, time\n",
    "from collections import Counter\n",
    "from werkzeug.utils import secure_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googlesearch import search\n",
    "# to search\n",
    "query = 'six of crows' + ' goodreads'\n",
    "searched_url = next(search(query, num=1, stop=0,  pause=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.goodreads.com/book/show/23437156-six-of-crows'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searched_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T hey stayed up planning well past midnight. Kaz was wary of the changes to the plan as well as the prospect of managing Nina’s pack of Grisha. But though he gave no indication to the others, there were elements of this new course that appealed to him. It was possible that Van Eck would piece together what the Shu were doing and go after the city’s remaining Grisha himself. They were a weapon Kaz didn’t want to see in the mercher’s arsenal.',\n",
       " 'But they couldn’t let this little rescue slow them down. With so many opponents and the stadwatch involved, they couldn’t afford it. Given enough time, the Shu would stop worrying about those dry-docked warships and the Council of Tides, and find their way to Black Veil. Kaz wanted Kuwei out of the city and removed from play as soon as possible.',\n",
       " 'At last, they put their lists and sketches aside. The wreckage of their makeshift meal was cleared from the table to avoid attracting the rats of Black Veil, and the lanterns were doused.',\n",
       " 'The others would sleep. Kaz could not. He’d meant what he’d said. Van Eck had more money, more allies, and the might of the city behind him. They couldn’t just be smarter than Van Eck, they had to be relentless. And Kaz could see what the others couldn’t. They’d won the battle today; they’d set out to get Inej back from Van Eck and they had. But the merch was still winning the war.',\n",
       " 'That Van Eck was willing to risk involving the stadwatch , and by extension the Merchant Council, meant he really believed he was invulnerable. Kaz still had the note Van Eck had sent arranging the meeting on Vellgeluk, but it was shoddy proof of the man’s schemes. He remembered what Pekka Rollins had said back at the Emerald Palace, when Kaz had claimed that the Merchant Council would never stand for Van Eck’s illegal activities. And who’s going to tell them? A canal rat from the worst slum in the Barrel? Don’t kid yourself, Brekker.',\n",
       " 'At the time, Kaz had barely been able to think beyond the red haze of anger that descended when he was in Rollins’ presence. It stripped away the reason that guided him, the patience he relied on. Around Pekka, he lost the shape of who he was—no, he lost the shape of who he’d fought to become. He wasn’t Dirtyhands or Kaz Brekker or even the toughest lieutenant in the Dregs. He was just a boy fueled by a white flame of rage, one that threatened to burn the pretense of the hard-won civility he maintained to ash.',\n",
       " 'But now, leaning on his cane among the graves of Black Veil, he could acknowledge the truth of Pekka’s words. You couldn’t go to war with an upstanding merch like Van Eck, not if you were a thug with a reputation dirtier than a stable hand’s boot sole. To win, Kaz would have to level the field. He would show the world what he already knew: Despite his soft hands and fine suits, Van Eck was a criminal, just as bad as any Barrel thug—worse, because his word was worth nothing.',\n",
       " 'Kaz didn’t hear Inej approach, he just knew when she was there, standing beside the broken columns of a white marble mausoleum. She’d found soap to wash with somewhere, and the scent of the dank rooms of Eil Komedie—that faint hint of hay and greasepaint—was gone. Her black hair shone in the moonlight, already tucked tidily away in a coil at her neck, and her stillness was so complete she might have been mistaken for one of the cemetery’s stone guardians.',\n",
       " '“Why the net, Kaz?”',\n",
       " 'Yes, why the net? Why something that would complicate the assault he’d planned on the silos and leave them twice as open to exposure? I couldn’t bear to watch you fall. “I just went to a lot of trouble to get my spider back. I didn’t do it so you could crack your skull open the next day.”',\n",
       " '“You protect your investments.” Her voice sounded almost resigned.',\n",
       " '“That’s right.”',\n",
       " '“And you’re going off island.”',\n",
       " 'He should be more concerned that she could guess his next move. “Rotty says the old man’s getting restless. I need to go smooth his feathers.”',\n",
       " 'Per Haskell was still the leader of the Dregs, and Kaz knew he liked the perks of that position, but not the work that went with it. With Kaz gone for so long, things would be starting to unravel. Besides, when Haskell got antsy, he liked to do something stupid just to remind people he was in charge.',\n",
       " '“We should get eyes on Van Eck’s house too,” said Inej.',\n",
       " '“I’ll take care of it.”',\n",
       " '“He’ll have strengthened his security.” The rest went unspoken. There was no one better equipped to slip past Van Eck’s defenses than the Wraith.',\n",
       " 'He should tell her to rest, tell her he would handle the surveillance on his own. Instead, he nodded and set out for one of the gondels hidden in the willows, ignoring the relief he felt when she followed.',\n",
       " 'After the raucous din of the afternoon, the canals seemed more silent than usual, the water unnaturally still.',\n",
       " '“Do you think West Stave will be back to itself tonight?” Inej asked, voice low. She’d learned a canal rat’s caution when it came to traveling the waterways of Ketterdam.',\n",
       " '“I doubt it. The stadwatch will be investigating, and tourists don’t come to Ketterdam for the thrill of being blown to bits.” A lot of businesses were going to lose money. Come tomorrow morning, Kaz suspected the front steps of the Stadhall would be crowded with the owners of pleasure houses and hotels demanding answers. Could be quite a scene. Good. Let the members of the Merchant Council concern themselves with problems other than Jan Van Eck and his missing son. “Van Eck will have changed things up since we lifted the DeKappel.”',\n",
       " '“And now that he knows Wylan is with us,” agreed Inej. “Where are we going to meet the old man?”',\n",
       " '“The Knuckle.”',\n",
       " 'They couldn’t intercept Haskell at the Slat. Van Eck would have been keeping the Dregs’ headquarters under surveillance, and now there were probably stadwatch swarming over it too. The thought of stadwatch grunts searching his rooms, digging through his few belongings, sent fury prickling over Kaz’s skin. The Slat wasn’t much, but Kaz had converted it from a leaky squat to a place you could sleep off a bender or lie low from the law without freezing your ass off in the winter or being bled by fleas in the summer. The Slat was his, no matter what Per Haskell thought.',\n",
       " 'Kaz steered the gondel into Zovercanal at the eastern edge of the Barrel. Per Haskell liked to hold court at the Fair Weather Inn on the same night every week, meeting up with his cronies to play cards and gossip. There was no way he’d miss it tonight, not when his favored lieutenant—his missing favored lieutenant—had fallen out with a member of the Merchant Council and brought so much trouble to the Dregs, not when he’d be the center of attention.',\n",
       " 'No windows faced onto the Knuckle, a crooked passage that bent between a tenement and a factory that manufactured cut-rate souvenirs. It was quiet, dimly lit, and so narrow it could barely call itself an alley—the perfect place for a jump. Though it wasn’t the safest route from the Slat to the Fair Weather, it was the most direct, and Per Haskell never could resist a shortcut.',\n",
       " 'Kaz moored the boat near a small footbridge and he and Inej took up their places in the shadows to wait, the need for silence understood. Less than twenty minutes later, a man’s silhouette appeared in the lamplight at the mouth of the alley, an absurd feather jutting from the crown of his hat.',\n",
       " 'Kaz waited until the figure was almost level with him before he stepped forward. “Haskell.”',\n",
       " 'Per Haskell whirled, pulling a pistol from his coat. He moved quickly despite his age, but Kaz had known he would be packing iron and was ready. He gave Haskell’s shoulder a quick jab with the tip of his cane, just enough to send a jolt of numbness to his hand.',\n",
       " 'Haskell grunted and the gun slipped from his grasp. Inej caught it before it could hit the ground and tossed it to Kaz.',\n",
       " '“Brekker,” Haskell said angrily, trying to wiggle his numb arm. “Where the hell have you been? And what kind of skiv rolls his own boss in an alley?”',\n",
       " '“I’m not robbing you. I just didn’t want you to shoot anyone before we had a chance to talk.” Kaz handed the gun back to Haskell by its grip. The old man snatched it from his palm, grizzled chin jutting out stubbornly.',\n",
       " '“Always overstepping,” he grumbled, tucking the weapon into a pocket of his nubbly plaid jacket, unable to reach his holster with his incapacitated arm. “You know what trouble you brought down on me today, boy?”',\n",
       " '“I do. That’s why I’m here.”',\n",
       " '“There were stadwatch crawling all over the Slat and the Crow Club. We had to shut the whole place down, and who knows when we’ll be able to start up again. What were you thinking, kidnapping a mercher’s son? This was the big job you left town for? The one supposed to make me wealthy beyond my wildest dreams?”',\n",
       " '“I didn’t kidnap anyone.” Not strictly true, but Kaz figured the subtleties would be lost on Per Haskell.',\n",
       " '“Then what in Ghezen’s name is going on?” Haskell whispered furiously, spittle flying. “You’ve got my best spider,” he said, gesturing to Inej. “My best shooter, my Heartrender, my biggest bruiser—”',\n",
       " '“Muzzen is dead.”',\n",
       " '“Son of a bitch,” Haskell swore. “First Big Bolliger, now Muzzen. You trying to gut my whole gang?”',\n",
       " '“No, sir.”',\n",
       " '“Sir. What are you about, boy?”',\n",
       " '“Van Eck is playing a fast game, but I’m still a step ahead of him.”',\n",
       " '“Don’t look like it from here.”',\n",
       " '“Good,” said Kaz. “Better no one sees us coming. Muzzen was a loss I didn’t anticipate, but give me a few more days and not only will the law be off your back, your coffers will be so heavy you’ll be able to fill your bathtub with gold and take a swim in it.”',\n",
       " 'Haskell’s eyes narrowed. “How much money are we talking?”',\n",
       " 'That’s the way , Kaz thought, watching greed light Haskell’s gaze, the lever at work.',\n",
       " '“Four million kruge. ”',\n",
       " 'Haskell’s eyes widened. A life of drink and hard living in the Barrel had turned the whites yellowy. “You trying to cozy me?”',\n",
       " '“I told you this was a big haul.”']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_dict = pd.read_pickle(r\"C:\\Users\\Lenovo\\Flask apps\\flask-app-character-net\\uploaded_files\\book_dict.pkl\")\n",
    "i=0\n",
    "all_sentences = []\n",
    "list_corpus = book_dict['book_content']\n",
    "list_corpus = [sent for sent in list_corpus.split('\\n') if sent !='']\n",
    "list_corpus[50*i:50*(i+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fantasy\n",
      "Young Adult\n",
      "Fiction\n",
      "Young Adult\n",
      "Young Adult Fantasy\n",
      "Adventure\n",
      "LGBT\n",
      "Fantasy\n",
      "Magic\n",
      "Romance\n",
      "Fantasy\n",
      "High Fantasy\n",
      "Audiobook\n"
     ]
    }
   ],
   "source": [
    "\n",
    "resp = requests.get(searched_url)\n",
    "\n",
    "soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "\n",
    "for x in soup.find_all(\"a\", { \"class\" : \"actionLinkLite bookPageGenreLink\" }):\n",
    "\n",
    "    print(x.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googlesearch import search\n",
    "from distutils.command.config import config\n",
    "from flask import Flask, flash, redirect, url_for, render_template, request, Response\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests, json, ast\n",
    "import os, io, csv, sys, pickle, time\n",
    "from collections import Counter\n",
    "from werkzeug.utils import secure_filename\n",
    "\n",
    "\n",
    "# to search\n",
    "query = 'six of crows' + ' goodreads'\n",
    "searched_url = next(search(query, num=1, stop=0,  pause=0))\n",
    "\n",
    "\n",
    "resp = requests.get(searched_url)\n",
    "soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "\n",
    "for x in soup.find_all(\"a\", { \"class\" : \"actionLinkLite bookPageGenreLink\" }):\n",
    "\n",
    "    print(x.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-1b4e1022a0e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAutoModelForTokenClassification\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"text-classification\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\pipelines\\__init__.py\u001b[0m in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, framework, revision, use_fast, use_auth_token, model_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m    419\u001b[0m         \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_from_pipeline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mconfig\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 421\u001b[1;33m         \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_from_pipeline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    422\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m     \u001b[0mmodel_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    513\u001b[0m         \"\"\"\n\u001b[0;32m    514\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"_from_auto\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 515\u001b[1;33m         \u001b[0mconfig_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    516\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m\"model_type\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m             \u001b[0mconfig_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCONFIG_MAPPING\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"model_type\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    522\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;31m# Load from URL or cache if already cached\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 524\u001b[1;33m             resolved_config_file = cached_path(\n\u001b[0m\u001b[0;32m    525\u001b[0m                 \u001b[0mconfig_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    526\u001b[0m                 \u001b[0mcache_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[1;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[0;32m   1402\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mis_remote_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1403\u001b[0m         \u001b[1;31m# URL, so get it from the cache (downloading if necessary)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1404\u001b[1;33m         output_path = get_from_cache(\n\u001b[0m\u001b[0;32m   1405\u001b[0m             \u001b[0murl_or_filename\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1406\u001b[0m             \u001b[0mcache_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[1;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[0;32m   1625\u001b[0m                     )\n\u001b[0;32m   1626\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1627\u001b[1;33m                     raise ValueError(\n\u001b[0m\u001b[0;32m   1628\u001b[0m                         \u001b[1;34m\"Connection error, and we cannot find the requested files in the cached path.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1629\u001b[0m                         \u001b[1;34m\" Please try again or make sure your Internet connection is on.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on."
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer\n",
    "\n",
    "classifier = pipeline(\"text-classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-2d8728eeeb68>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"bert-base-cased-finetuned-mrpc\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m         \u001b[1;31m# First, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 428\u001b[1;33m         \u001b[0mtokenizer_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_tokenizer_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    429\u001b[0m         \u001b[0mconfig_tokenizer_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer_config\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tokenizer_class\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py\u001b[0m in \u001b[0;36mget_tokenizer_config\u001b[1;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, **kwargs)\u001b[0m\n\u001b[0;32m    319\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;31m# Load from URL or cache if already cached\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m         resolved_config_file = cached_path(\n\u001b[0m\u001b[0;32m    322\u001b[0m             \u001b[0mconfig_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m             \u001b[0mcache_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[1;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[0;32m   1402\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mis_remote_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1403\u001b[0m         \u001b[1;31m# URL, so get it from the cache (downloading if necessary)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1404\u001b[1;33m         output_path = get_from_cache(\n\u001b[0m\u001b[0;32m   1405\u001b[0m             \u001b[0murl_or_filename\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1406\u001b[0m             \u001b[0mcache_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[1;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[0;32m   1625\u001b[0m                     )\n\u001b[0;32m   1626\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1627\u001b[1;33m                     raise ValueError(\n\u001b[0m\u001b[0;32m   1628\u001b[0m                         \u001b[1;34m\"Connection error, and we cannot find the requested files in the cached path.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1629\u001b[0m                         \u001b[1;34m\" Please try again or make sure your Internet connection is on.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on."
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated git hooks.\n",
      "Git LFS initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'distilbert-base-uncased-finetuned-sst-2-english'...\n",
      "fatal: unable to access 'https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english/': OpenSSL SSL_connect: Connection was reset in connection to huggingface.co:443 \n"
     ]
    }
   ],
   "source": [
    "!git lfs install\n",
    "!git clone https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not open requirements file: [Errno 2] No such file or directory: '/content/character-network/requirements_sentiment_analysis.txt'\n"
     ]
    }
   ],
   "source": [
    "!pip install -r /content/character-network/requirements_sentiment_analysis.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'C:\\Users\\Lenovo\\flask-app-character-net\\first_book_props\\book_content.pkl', 'rb') as f:\n",
    "    book_content = f.read()\n",
    "with open(r'C:\\Users\\Lenovo\\flask-app-character-net\\first_book_props\\sentiment_lables.pkl', 'rb') as f:\n",
    "    sentiment_lables = f.read()\n",
    "with open(r'C:\\Users\\Lenovo\\flask-app-character-net\\first_book_props\\encoded_sentiment_labels.pkl', 'rb') as f:\n",
    "    encoded_sentiment_labels = f.read()\n",
    "with open(r'C:\\Users\\Lenovo\\flask-app-character-net\\first_book_props\\emotions_count.pkl', 'r') as f:\n",
    "    emotions_count = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'POSITIVE': 2530, 'NEGATIVE': 3896, 'NEUTRAL': 258}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_pickle(r'C:\\Users\\Lenovo\\flask-app-character-net\\first_book_props\\emotions_count.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'€\\x03}q\\x00(X\\x08\\x00\\x00\\x00POSITIVEq\\x01Mâ\\tX\\x08\\x00\\x00\\x00NEGATIVEq\\x02M8\\x0fX\\x07\\x00\\x00\\x00NEUTRALq\\x03M\\x02\\x01u.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment analysis on sentences\n",
    "# we use gpu for this task\n",
    "\n",
    "def senti_analysis_transformers(sentences, plot=True):\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    given the fact that we have GPU on colab\n",
    "    We empty the cache and initialize our sentiment analysis\n",
    "    \"\"\"\n",
    "    print('Defining the sentiment of each sentence using a semantic model...')\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    sentiments_lists = []\n",
    "\n",
    "    # we use small batches to prevent crashing\n",
    "    for i in range(0, len(sentences), 50):\n",
    "        if i%1000==0 : print('sentence', colored(f\"{i}\", 'blue'))\n",
    "        sentiments_lists.append(classifier(sentences[i: i+50]))\n",
    "    \n",
    "    # list of list -> list\n",
    "    all_sentences_sentiment = list(chain.from_iterable(sentiments_lists))\n",
    "\n",
    "    labels, encoded_labels = decide_for_sentiment_label(all_sentences_sentiment)\n",
    "\n",
    "    \n",
    "    # count the labels\n",
    "    \n",
    "    emotions_count = dict(Counter(labels))\n",
    "    print(colored('Emotions dominance: \\n', 'blue'), emotions_count)\n",
    "    \n",
    "    if plot:plot_emotions(emotions_count)\n",
    "    \n",
    "    return labels, encoded_labels, emotions_count\n",
    "\n",
    "sentiment_lables, encoded_sentiment_labels, emotions_count = senti_analysis_transformers(book_sents_cleaned, plot=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from afinn import Afinn\n",
    "\n",
    "afn = Afinn()\n",
    "\n",
    "afn.score('we are kind to good people')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    sent_dict = {}\n",
    "    afinn = Afinn()\n",
    "    for sent in sentence_list:\n",
    "        sent_dict[sent] = afinn.score(sent)\n",
    "\n",
    "    #sentiment_score = [x: afinn.score(x) for x in sentence_list]\n",
    "    #align_rate = np.sum(sentiment_score)/len(np.nonzero(sentiment_score)[0]) * -2\n",
    "    sent_df = pd.DataFrame({'sentiment': sent_dict.values(),\n",
    " 'sentence': sent_dict.keys()})\n",
    "    return sent_dict, sent_df\n",
    "\n",
    "sent_dict, sent_df = calculate_align_rate(book_1_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-494b13eaf22e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m             \u001b[1;32melse\u001b[0m \u001b[1;34m'negative'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m             \u001b[1;32melse\u001b[0m \u001b[1;34m'neutral'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m             for score in scores]\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'scores' is not defined"
     ]
    }
   ],
   "source": [
    "sentiment = ['positive' if score > 0 \n",
    "            else 'negative' if score < 0 \n",
    "            else 'neutral' \n",
    "            for score in scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['NEUTRAL', 'NEGATIVE'], [0.0, -0.8], {'NEUTRAL': 1, 'NEGATIVE': 1})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from afinn import Afinn\n",
    "\n",
    "afn = Afinn()\n",
    "senti_analysis_Afinn(['this is ok', 'fucked up.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def senti_analysis_Afinn(sentence_list:list):\n",
    "    '''\n",
    "    Function to calculate the align_rate of the whole novel\n",
    "    :param sentence_list: the list of sentence of the whole novel.\n",
    "    :return: the align rate of the novel.\n",
    "    '''\n",
    "    afn = Afinn()\n",
    "\n",
    "    # encoded_labels = sentiments_lists in transformers\n",
    "    encoded_labels = []\n",
    "\n",
    "    # the score is divided to be compatible with the transformers approach\n",
    "    # |max or min Afinn| = 5\n",
    "    for sent in sentence_list:\n",
    "        encoded_labels.append(afn.score(sent)/5)\n",
    "    \n",
    "\n",
    "    labels = sentiment = [\n",
    "        'POSITIVE' if score > 0 \n",
    "        else 'NEGATIVE' if score < 0 \n",
    "        else 'NEUTRAL' \n",
    "        for score in encoded_labels\n",
    "        ]\n",
    "\n",
    "    \n",
    "    # count the labels\n",
    "    emotions_count = dict(Counter(labels))\n",
    "    #print(colored('Emotions dominance: \\n', 'blue'), emotions_count)\n",
    "    \n",
    "    #if plot:plot_emotions(emotions_count)\n",
    "    \n",
    "    return labels, encoded_labels, emotions_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>top</th>\n",
       "      <th>this</th>\n",
       "      <th>that</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>sdfsd</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>sdfsg</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   top   this  that\n",
       "0    0  sdfsd     5\n",
       "1    1  sdfsg    10"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame({'Top':i, 'Known as':k, 'Num. of Appearances':v} for i, (k,v) in enumerate({'sdfsd':5, 'sdfsg': 10}.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_for_transformer_sentiment_label(self, senti_dicts:dict) ->list:\n",
    "    \"\"\"\n",
    "    takes in dictionaries of sentiment analysis\n",
    "    in this format: {'label': '...', 'score':...} \n",
    "    (label: 'POSITIVE'/'NEGATIVE', 'score': 0<s<1)\n",
    "    outputs a list representing the emotion of each sentence\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "\n",
    "    for senti_dict in senti_dicts:\n",
    "\n",
    "        # senti_dict['score'] = how certain the model is about an emotion\n",
    "        if float(senti_dict['score'])< 0.7:\n",
    "            labels.append('NEUTRAL')\n",
    "\n",
    "        else: labels.append(senti_dict['label'])\n",
    "\n",
    "    # encode the labels\n",
    "    emotions_label = {'POSITIVE': 1, 'NEUTRAL':0, 'NEGATIVE':-1}\n",
    "\n",
    "    # encode the three types\n",
    "    encoded_lables = [emotions_label[em] for em in labels]\n",
    "    \n",
    "    \n",
    "\n",
    "    return labels, encoded_lables\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# sentiment analysis on sentences\n",
    "# we use gpu for this task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# new phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import codecs\n",
    "import os\n",
    "import spacy\n",
    "import json\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from afinn import Afinn\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import plotly.express as px\n",
    "import spacy\n",
    "from termcolor import colored\n",
    "import time\n",
    "from itertools import chain \n",
    "import string\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm \n",
    "import tqdm.notebook as tq\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sys.path.append(r'C:\\Users\\Lenovo\\character-network')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from character_net_src import Book_analyzer\n",
    "\n",
    "analyzer = Book_analyzer()\n",
    "\n",
    "\n",
    "book_content_cleaned = analyzer.clean_sentences(\"\"\"They’d wanted him to die without honor, murdered in an infirmary bed.\n",
    "Despite the fact that she’d been as good as starving herself, the vent was still a tight fit.\n",
    "She’d woken sometime after the fight on Vellgeluk, with no sense of how long she’d been unconscious and no idea where she was.\n",
    "She’d managed to nudge her blindfold down by scraping her face against the wall.\n",
    "She’d leashed it by controlling her breath, in through the nose, out through the mouth, letting her mind turn to prayer as her Saints gathered around her.\n",
    "We meet fear , he’d said.\n",
    "He was just like the boys she’d grown up with, a head full of nonsense and a mouth full of easy charm.\n",
    "She’d fight with everything she had to free Inej even if she was still in the grips of parem .\n",
    "Van Eck had sent Bajan to her every day, and he’d been nothing but amiable and pleasant even as he’d prodded her for the locations of Kaz’s safe houses.\n",
    "Or maybe he thought she’d be more vulnerable to a Suli boy than a wily merch.\n",
    "By the time they’d made it to the main street, he had a respectable little bunch.\n",
    "Then he felt a little like the first time he’d tried brandy and ended up spewing his dinner all over his own shoes.\n",
    "“You’d be surprised,” said Jesper.\n",
    "Wylan clutched his side, wishing he’d shoved Jesper out of the wagon after all and jumped right down with him.\n",
    "But they’d come all this way.\"\"\")\n",
    "\n",
    "book_sentences = analyzer.spacy_detect_sentences(book_content_cleaned)\n",
    "\n",
    "finalized_sents = analyzer.clean_sentences(book_sentences, chapter_regex = 'No chapter')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(f\" *{string.punctuation}\\w *\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " *!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\\w *\n"
     ]
    }
   ],
   "source": [
    "print(f\" *{string.punctuation}\\w *\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['’d ']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(pattern=f\" *[{string.punctuation}’”]\\w *\",\n",
    " string=\"They’d wanted him to die without honor, murdered in an infirmary bed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_1 = r\"C:\\Users\\Lenovo\\character-network\\novels\\Harry Potter 1.txt\"\n",
    "path_2 = r'C:\\Users\\Lenovo\\Desktop\\croocked.txt'\n",
    "\n",
    "from charset_normalizer import from_path\n",
    "\n",
    "results = from_path(path_2)\n",
    "\n",
    "book_content = str(results.best())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc object text (doc.text) =  They’d wanted him to die without honor, murdered in an infirmary bed.\n",
      "name detected by spacy (ent.text) =  ’d\n",
      "\u001b[32mfound!\u001b[0m\n",
      "doc object text (doc.text) =  She’d managed to nudge her blindfold down by scraping her face against the wall.\n",
      "name detected by spacy (ent.text) =  ’d\n",
      "\u001b[32mfound!\u001b[0m\n",
      "doc object text (doc.text) =  She’d leashed it by controlling her breath, in through the nose, out through the mouth, letting her mind turn to prayer as her Saints gathered around her.\n",
      "name detected by spacy (ent.text) =  ’d\n",
      "\u001b[32mfound!\u001b[0m\n",
      "doc object text (doc.text) =  She’d fight with everything she had to free Inej even if she was still in the grips of parem .\n",
      "name detected by spacy (ent.text) =  ’d\n",
      "\u001b[32mfound!\u001b[0m\n",
      "doc object text (doc.text) =  Van Eck had sent Bajan to her every day, and he’d been nothing but amiable and pleasant even as he’d prodded her for the locations of Kaz’s safe houses.\n",
      "name detected by spacy (ent.text) =  Van Eck\n",
      "doc object text (doc.text) =  Van Eck had sent Bajan to her every day, and he’d been nothing but amiable and pleasant even as he’d prodded her for the locations of Kaz’s safe houses.\n",
      "name detected by spacy (ent.text) =  ’d\n",
      "\u001b[32mfound!\u001b[0m\n",
      "doc object text (doc.text) =  Or maybe he thought she’d be more vulnerable to a Suli boy than a wily merch.\n",
      "name detected by spacy (ent.text) =  Suli\n",
      "doc object text (doc.text) =  “You’d be surprised,” said Jesper.\n",
      "name detected by spacy (ent.text) =  Jesper\n",
      "doc object text (doc.text) =  Wylan clutched his side, wishing he’d shoved Jesper out of the wagon after all and jumped right down with him.\n",
      "name detected by spacy (ent.text) =  Wylan\n"
     ]
    }
   ],
   "source": [
    "# ner -----------------------------------------\n",
    "n=10\n",
    "sorted_flatten_names_dict = analyzer.flatten_pop_names(list_sents=finalized_sents)\n",
    "df = pd.DataFrame({'Top':i, 'Known as':k, 'Num. of Appearances':v} for i, (k,v) in enumerate(sorted_flatten_names_dict.items()))\n",
    "top_n_df = df.iloc[:n, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Top</th>\n",
       "      <th>Known as</th>\n",
       "      <th>Num. of Appearances</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Van Eck</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Suli</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Jesper</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Wylan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Top Known as  Num. of Appearances\n",
       "0    0  Van Eck                    1\n",
       "1    1     Suli                    1\n",
       "2    2   Jesper                    1\n",
       "3    3    Wylan                    1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_n_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c5e48f84046969b800ff52f6d80523bcd1ca3fb1a99f1449e4197bf6c73dc096"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
